{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Web Scrapping and Pre-Processing](#1.-Web-Scrapping-and-Pre-Processing)\n",
    "    - [1.1 Importing Libraries](#1.1-Importing-Libraries)\n",
    "    - [1.2 Scrapping 'AskMen' Subreddit](#1.2-Scrapping-'AskMen'-Subreddit)\n",
    "        - [1.2.1 Pre-work](#1.2.1-Pre-work)\n",
    "        - [1.2.2 Scrapping](#1.2.2-Scrapping)\n",
    "    - [1.3 Scrapping 'AskWomen' Subreddit](#1.3-Scrapping-'AskWomen'-Subreddit)\n",
    "        - [1.3.1 Pre-work](#1.3.1-Pre-work)\n",
    "        - [1.3.2 Scrapping](#1.3.2-Scrapping)\n",
    "    - [1.4 Merging Scrapping Results into a Data Frame for Future Processing](#1.4-Merging-Scrapping-Results-into-a-Data-Frame-for-Future-Processing)\n",
    "- [2. Exploratory Data Analysis](#2.-Exploratory-Data-Analysis)\n",
    "- [3. Baseline Model](#3.-Baseline-Model)\n",
    "- [4. Modeling](#4.-Modeling)\n",
    "    - [4.1 Logistic Regression](#4.1-Logistic-Regression)\n",
    "    - [4.2 Naive Bayes Model](#4.2-Naive-Bayes-Model)\n",
    "    - [4.3 Decision Trees Classifier](#4.3-Decision-Trees-Classifier)\n",
    "    - [4.4 Bagging Classifier](#4.4-Bagging-Classifier)\n",
    "    - [4.5 Random Forest Classifier](#4.5-Random-Forest-Classifier)\n",
    "    - [4.6 Extra Trees Classifier](#4.6-Extra-Trees-Classifier)\n",
    "    - [4.7 AdaBoost Classifier](#4.7-AdaBoost-Classifier)\n",
    "    - [4.8 Gradient Boosting Classifier](#4.8-Gradient-Boosting-Classifier)\n",
    "    - [4.9 Support Vector Machines](#4.9-Support-Vector-Machines)\n",
    "- [5. Conclusions](#5.-Conclusions)\n",
    "- [6. Recommendations](#6.-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Web Scrapping and Pre-Processing #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Importing Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from sklearn.pipeline                import Pipeline\n",
    "from sklearn.model_selection         import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.ensemble                import BaggingClassifier,RandomForestClassifier,ExtraTreesClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction      import stop_words\n",
    "from sklearn.naive_bayes             import GaussianNB, MultinomialNB\n",
    "from sklearn.tree                    import DecisionTreeClassifier\n",
    "from sklearn.svm                     import SVC\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Scrapping 'AskMen' Subreddit ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Pre-work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting constants\n",
    "url_base = 'https://www.reddit.com/r/AskMen.json'\n",
    "user_agent = {\"User-agent\": 'ilya-k'}\n",
    "after = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test run\n",
    "res = requests.get(url = url_base,\n",
    "                   headers = user_agent)\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Scrapping ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pull number 1\n",
      "Pull number 2\n",
      "Pull number 3\n",
      "Pull number 4\n",
      "Pull number 5\n",
      "Pull number 6\n",
      "Pull number 7\n",
      "Pull number 8\n",
      "Pull number 9\n",
      "Pull number 10\n",
      "Pull number 11\n",
      "Pull number 12\n",
      "Pull number 13\n",
      "Pull number 14\n",
      "Pull number 15\n",
      "Pull number 16\n",
      "Pull number 17\n",
      "Pull number 18\n",
      "Pull number 19\n",
      "Pull number 20\n"
     ]
    }
   ],
   "source": [
    "#This chunk of code was adopted from Boom D.\n",
    "\n",
    "#Scrapping the 'AskMen' subreddit\n",
    "\n",
    "#Instantiating an empty list to store the results\n",
    "posts=[]\n",
    "\n",
    "#Looping through the subreddit content\n",
    "for pull_num in range(20):\n",
    "    \n",
    "    #Status info print\n",
    "    print(f'Pull number {pull_num+1}')\n",
    "    \n",
    "    #First pull\n",
    "    if after == None:\n",
    "        new_url = url_base\n",
    "    \n",
    "    #Other pulls\n",
    "    else:\n",
    "        new_url = url_base + '?after=' + after\n",
    "  \n",
    "    #Pull result\n",
    "    res = requests.get(new_url,\n",
    "                headers = user_agent)\n",
    "\n",
    "    #Result processing\n",
    "    if res.status_code == 200:\n",
    "        json_data = res.json()\n",
    "        for post in json_data['data']['children']:\n",
    "            #Adding individual posts to the results list\n",
    "            posts.extend([post['data']['selftext']])\n",
    "    #Error protection   \n",
    "    else:\n",
    "        print(\"We've run into an error. The status code is:\", res.status_code)\n",
    "        break\n",
    "    \n",
    "    #Resetting the last post's indicator                           \n",
    "    after = json_data['data']['after']\n",
    "    \n",
    "    #Additional protection for API secutity and stability\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No one cares that you like listening to Taylor...</td>\n",
       "      <td>AskMen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Use this for all your Halloween discussion, su...</td>\n",
       "      <td>AskMen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>AskMen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>AskMen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>AskMen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  source\n",
       "0  No one cares that you like listening to Taylor...  AskMen\n",
       "1  Use this for all your Halloween discussion, su...  AskMen\n",
       "2                                                     AskMen\n",
       "3                                                     AskMen\n",
       "4                                                     AskMen"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Storing the scrapping results into a Data Frame\n",
    "data_men = pd.DataFrame(posts)\n",
    "\n",
    "#Setting a source indicator\n",
    "data_men['source'] = 'AskMen'\n",
    "\n",
    "#Setting the column names\n",
    "data_men.columns = ['text','source']\n",
    "\n",
    "# \"Meeting\" new DataFrame\n",
    "data_men.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could notice, there're empty posts in the Data Frame. As a part of Pre-processing process let's eradicate empty posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keepiing only meaningful posts in our Data Frame\n",
    "data_men = data_men[data_men['text'] !='']\n",
    "\n",
    "#Storing the results in a csv file\n",
    "data_men.to_csv('../data/AskMen.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Scrapping 'AskWomen' Subreddit ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Pre-work ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting constants\n",
    "url_base = 'https://www.reddit.com/r/AskWomen.json'\n",
    "user_agent = {\"User-agent\": 'ilya-k'}\n",
    "after = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test run\n",
    "res = requests.get(url = url_base,\n",
    "                   headers = user_agent)\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Scrapping ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pull number 1\n",
      "Pull number 2\n",
      "Pull number 3\n",
      "Pull number 4\n",
      "Pull number 5\n",
      "Pull number 6\n",
      "Pull number 7\n",
      "Pull number 8\n",
      "Pull number 9\n",
      "Pull number 10\n",
      "Pull number 11\n",
      "Pull number 12\n",
      "Pull number 13\n",
      "Pull number 14\n",
      "Pull number 15\n",
      "Pull number 16\n",
      "Pull number 17\n",
      "Pull number 18\n",
      "Pull number 19\n",
      "Pull number 20\n"
     ]
    }
   ],
   "source": [
    "#This chunk of code was adopted from Boom D.\n",
    "\n",
    "#Scrapping the 'AskWomen' subreddit\n",
    "\n",
    "#Instantiating an empty list to store the results\n",
    "posts=[]\n",
    "\n",
    "\n",
    "#Looping through the subreddit content\n",
    "for pull_num in range(20):\n",
    "    \n",
    "    #Status info print\n",
    "    print(f'Pull number {pull_num+1}')\n",
    "    \n",
    "    #First pull\n",
    "    if after == None:\n",
    "        new_url = url_base\n",
    "    \n",
    "    #Other pulls\n",
    "    else:\n",
    "        new_url = url_base + '?after=' + after\n",
    "  \n",
    "    #Pull result\n",
    "    res = requests.get(new_url,\n",
    "                headers = user_agent)\n",
    "\n",
    "    #Result processing\n",
    "    if res.status_code == 200:\n",
    "        json_data = res.json()\n",
    "        for post in json_data['data']['children']:\n",
    "            #Adding individual posts to the results list\n",
    "            posts.extend([post['data']['selftext']])\n",
    "    #Error protection   \n",
    "    else:\n",
    "        print(\"We've run into an error. The status code is:\", res.status_code)\n",
    "        break\n",
    "    \n",
    "    #Resetting the last post's indicator                           \n",
    "    after = json_data['data']['after']\n",
    "    \n",
    "    #Additional protection for API secutity and stability\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>For those of you who love (or hate!) this holi...</td>\n",
       "      <td>AskWomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\\nEvery Friday, just say whatever is in your m...</td>\n",
       "      <td>AskWomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Borrowed from the Ask Men sub but I loved all ...</td>\n",
       "      <td>AskWomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>AskWomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>I'm mainly talking emotional support</td>\n",
       "      <td>AskWomen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    source\n",
       "0  For those of you who love (or hate!) this holi...  AskWomen\n",
       "1  \\nEvery Friday, just say whatever is in your m...  AskWomen\n",
       "2  Borrowed from the Ask Men sub but I loved all ...  AskWomen\n",
       "3                                                     AskWomen\n",
       "4               I'm mainly talking emotional support  AskWomen"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Storing the scrapping results into a Data Frame\n",
    "data_women = pd.DataFrame(posts)\n",
    "\n",
    "#Setting a source indicator\n",
    "data_women['source'] = 'AskWomen'\n",
    "\n",
    "#Setting the column names\n",
    "data_women.columns = ['text','source']\n",
    "\n",
    "# \"Meeting\" new DataFrame\n",
    "data_women.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keepiing only meaningful posts in our Data Frame\n",
    "data_women = data_women[data_women['text'] !='']\n",
    "\n",
    "#Storing the results in a csv file\n",
    "data_women.to_csv('../data/AskWomen.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Merging Scrapping Results into a Data Frame for Future Processing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining our two Data Frames for the two subreddits into one Data Frame for future processing\n",
    "data_joined = pd.concat([data_men,data_women])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No one cares that you like listening to Taylor...</td>\n",
       "      <td>AskMen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Use this for all your Halloween discussion, su...</td>\n",
       "      <td>AskMen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>I don’t have friends in my new city. My family...</td>\n",
       "      <td>AskMen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>I am curious for comparison because I have had...</td>\n",
       "      <td>AskMen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Saw that recent post about what your partner d...</td>\n",
       "      <td>AskMen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>There is so much more to relationships than ph...</td>\n",
       "      <td>AskWomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>Edit: As in a personal story or a memory.</td>\n",
       "      <td>AskWomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>And have you ever caught yourself objectifying...</td>\n",
       "      <td>AskWomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>If  you had a magical notebook in which anythi...</td>\n",
       "      <td>AskWomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>UPDATE: I got invited to an in-person intervie...</td>\n",
       "      <td>AskWomen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>383 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text    source\n",
       "0    No one cares that you like listening to Taylor...    AskMen\n",
       "1    Use this for all your Halloween discussion, su...    AskMen\n",
       "8    I don’t have friends in my new city. My family...    AskMen\n",
       "9    I am curious for comparison because I have had...    AskMen\n",
       "12   Saw that recent post about what your partner d...    AskMen\n",
       "..                                                 ...       ...\n",
       "492  There is so much more to relationships than ph...  AskWomen\n",
       "493          Edit: As in a personal story or a memory.  AskWomen\n",
       "494  And have you ever caught yourself objectifying...  AskWomen\n",
       "496  If  you had a magical notebook in which anythi...  AskWomen\n",
       "501  UPDATE: I got invited to an in-person intervie...  AskWomen\n",
       "\n",
       "[383 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\"Meeting\" the new joined Data Frame\n",
    "data_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving our new joined Data Frame to csv file\n",
    "data_joined.to_csv('../data/data_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No one cares that you like listening to Taylor...</td>\n",
       "      <td>AskMen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Use this for all your Halloween discussion, su...</td>\n",
       "      <td>AskMen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>I don’t have friends in my new city. My family...</td>\n",
       "      <td>AskMen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>I am curious for comparison because I have had...</td>\n",
       "      <td>AskMen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>Saw that recent post about what your partner d...</td>\n",
       "      <td>AskMen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  source\n",
       "0           0  No one cares that you like listening to Taylor...  AskMen\n",
       "1           1  Use this for all your Halloween discussion, su...  AskMen\n",
       "2           8  I don’t have friends in my new city. My family...  AskMen\n",
       "3           9  I am curious for comparison because I have had...  AskMen\n",
       "4          12  Saw that recent post about what your partner d...  AskMen"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing data\n",
    "data = pd.read_csv('../data/data_raw.csv')\n",
    "\n",
    "#\"Meeting\" our new Data Frame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping a column\n",
    "data.drop(columns = ['Unnamed: 0'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      0\n",
       "source    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(383, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exploring the shape of our data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      object\n",
       "source    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exploring data types\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AskMen      253\n",
       "AskWomen    130\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identifying our classes\n",
    "data['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we will need to introduce stratification into our future training and testing sets splits, as we might encounter unbalanced classes, especially with testing sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Baseline Model #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to work on classification models, we will need to set our Baseline Model on our prevailing class - it will be 'AskMen' with the 253 values against 'AskWomen' class with 130 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AskMen      0.660574\n",
       "AskWomen    0.339426\n",
       "Name: source, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['source'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hence, our Baselline Model's accuracy score is 0.6606 - this is the accuracy score we are going to evaluate all our future models against.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Logistic Regression ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to look for optimal parameters for our Logistic Regression model, we will use a pipeline with an optimizer (count vectorizer and TD-IDF vectorizer) and our logistic regression as a model and perform a grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features matrix and target vector\n",
    "X = data['text']\n",
    "y = data['source']\n",
    "\n",
    "#Training and testing sets split with stratification \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating two vectorizers\n",
    "cvec = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "#Instantiating a pipeline for CountVectorizer\n",
    "pipe_cvec = Pipeline(steps = [('vectorizer', CountVectorizer()), \n",
    "                        ('model', LogisticRegression())])\n",
    "\n",
    "#Setting grid search parameters\n",
    "hyperparams_cvec = {'vectorizer__max_features':[1000,2500,5000],\n",
    "                    'vectorizer__ngram_range':[(1,1),(2,2),(1,2), (1,3),(2,3),(3,3)],\n",
    "                    'vectorizer__stop_words':[None, 'english']\n",
    "                   }\n",
    "\n",
    "#Instantiating a pipeline for TF-IDF Vectorizer\n",
    "pipe_tfidf = Pipeline(steps = [('vectorizer', TfidfVectorizer()), \n",
    "                        ('model', LogisticRegression())])\n",
    "\n",
    "#Setting grid search parameters\n",
    "hyperparams_tfidf = {'vectorizer__max_features':[1000,2500,5000],\n",
    "                    'vectorizer__ngram_range':[(1,1),(2,2),(1,2), (1,3),(2,3),(3,3)],\n",
    "                    'vectorizer__stop_words':[None, 'english'],\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilyakvyatkovskiy/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Instantiating grid search with 3-fold cross-validation\n",
    "gs_cvec = GridSearchCV(pipe_cvec,\n",
    "                      hyperparams_cvec,\n",
    "                      cv=3)\n",
    "gs_tfidf = GridSearchCV(pipe_tfidf,\n",
    "                       hyperparams_tfidf,\n",
    "                       cv=3)\n",
    "\n",
    "#Fitting grid search\n",
    "results_cvec = gs_cvec.fit(X_train,y_train)\n",
    "results_tfidf = gs_tfidf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7456445993031359"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best accuracy score for a CountVectorizer pipeline\n",
    "results_cvec.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9965156794425087"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best accuracy on the training set for a CountVectorizer pipeline\n",
    "results_cvec.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7038327526132404"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best accuracy score for a TD-IDF pipeline\n",
    "results_tfidf.best_score_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8780487804878049"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best accuracy on the training set for a TF-IDF pipeline\n",
    "results_tfidf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see, the resulting accuracy for CountVectorizer is better than for TD-IDF Vectorizer. Hence, our best Logistic Regression modelf uses the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorizer__max_features': 2500,\n",
       " 'vectorizer__ngram_range': (1, 1),\n",
       " 'vectorizer__stop_words': None}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best estimator for CountVectorizer\n",
    "results_cvec.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our best Logistic Regression model has an accuracy score of 0.7456 (in comparison with the Baseline Model's accuracy of 0.6606. All Logistic regression models are quite overfitted on training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Naive Bayes Model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of Naive Bayes model depends on our feature matrix, which is largely dependent on our choice of vectorizer. \n",
    "With Count Vectorizer our features matrix contains only positive integer values - hence the Naive Bayes Multinomial model will be the most appropriate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating model\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "#Instantiating Count Vectorizer\n",
    "cvec=CountVectorizer()\n",
    "\n",
    "#Vectorizing Transforming our features matriz\n",
    "X_mnb = cvec.fit_transform(X)\n",
    "\n",
    "#Trasforming our sparse matrix to array\n",
    "X_mnb = X_mnb.toarray()\n",
    "\n",
    "#Train-test split with stratification for this particular method\n",
    "X_train_mnb, X_test_mnb, y_train_mnb, y_test_mnb = train_test_split(X_mnb, y, stratify=y, random_state=81)\n",
    "\n",
    "#Fitting the model\n",
    "result_mnb = mnb.fit(X_train_mnb,y_train_mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9024390243902439"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting model's accuracy on trainig set\n",
    "result_mnb.score(X_train_mnb, y_train_mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting model's accuracy on testingn set\n",
    "result_mnb.score(X_test_mnb, y_test_mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could notice, our Multinomial Naive Bayes model is very overfitted on training data, as it shows accuracy of 0.9024 on a training set, but just 0.75 on a testing set - which is for now our new highest score. It surpasses teh score for the optimized Logistic Regression just by 0.0044 which is kind of negligible, as both models are very overfit.\n",
    "\n",
    "With TF-IDF Vectorizer our features matrix contains only positive non-integer values - hence the Naive Bayes Gaussian model will be the most appropriate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "#Instantiating Count Vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "#Vectorizing and transforming our features matriz\n",
    "X_gnb = tfidf.fit_transform(X)\n",
    "\n",
    "#Trasforming our sparse matrix to array\n",
    "X_gnb = X_gnb.toarray()\n",
    "\n",
    "#Train-test split with stratification for this particlular model\n",
    "X_train_gau, X_test_gau, y_train_gau, y_test_gau = train_test_split(X_gnb, y, stratify=y, random_state=81)\n",
    "\n",
    "#Fitting the model\n",
    "result_gnb = gnb.fit(X_train_gau,y_train_gau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9860627177700348"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting model's accuracy on trainig set\n",
    "result_gnb.score(X_train_gau, y_train_gau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7083333333333334"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting model's accuracy on testingn set\n",
    "result_gnb.score(X_test_gau, y_test_gau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could notice, our Gaussian Naive Bayes model is as well very overfitted on training data, as it shows accuracy of 0.9861 on a training set, but just 0.7083 on a testing set. \n",
    "But the accuracy on the testing score has not improved in comparison with the situation when we used Count Vectorizer and Multinomial Naive Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Decision Trees Classifier ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try how a Decision Tree Classifier works on text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the model\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "#Instantiating Count Vectorizer\n",
    "cvec = CountVectorizer()\n",
    "\n",
    "#Vectorizing and transforming our features matrix\n",
    "X_train_cvec=cvec.fit_transform(X_train)\n",
    "X_test_cvec = cvec.transform(X_test)\n",
    "\n",
    "#Fitting the model\n",
    "result_tree_cvec = tree.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score training set\n",
    "result_tree_cvec.score(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65625"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score testing set\n",
    "result_tree_cvec.score(X_test_cvec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see, our Decision Tree model with Count Vectorizer is severely overfitted on training data again, with accuracy score for training set of 1.0 and for testing set - just 0.6525. Let's compare the results with the situation when we use TF-IDF Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "#Vectorizing and transforming our features matrix\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "#Fitting the model\n",
    "result_tree_tfidf = tree.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score training set\n",
    "result_tree_tfidf.score(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score testing set\n",
    "result_tree_tfidf.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy scores for our Decision Tree Calssifier model with TF-IDF Vectorizer show a similar picture: the model is highly overfit on training data with accuracy for the training set of 1.0 and just 0.625 for the testing set. \n",
    "\n",
    "Again, the same type of model shows better accuracy on testing set when we use Count Vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Bagging Classifier ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if using of a Bagging Classifier brings us to better scores. While preparing this model I dod a grid search on some model parameters, and it turned out that a model with using of Count Vectorizer with maximum features  of 2500,  n_gram of 1 token and not using any stop words, along with 111 estimators (trees) was the best. I am commenting the code for these results as running it takes a lot of time. I'm instantiating a model with these parameters in this notebook instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS FOR REFERENCE ONLY\n",
    "\n",
    "# pipe_bag = Pipeline(steps = [('vectorizer', CountVectorizer()), \n",
    "#                         ('model', BaggingClassifier())])\n",
    "\n",
    "# hyperparams_bag = {'vectorizer__max_features':[1000,2500,5000],\n",
    "#                     'vectorizer__ngram_range':[(1,1),(2,2),(1,2), (1,3),(2,3),(3,3)],\n",
    "#                     'vectorizer__stop_words':[None, 'english'],\n",
    "#                      'model__n_estimators':[11,33,55,77,99,111]\n",
    "#                    }\n",
    "\n",
    "# gs_bag = GridSearchCV(pipe_bag,\n",
    "#                       hyperparams_bag,\n",
    "#                       cv=3)\n",
    "# results_bag = gs_bag.fit(X_train,y_train)\n",
    "#  results_bag.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating vectorizer\n",
    "cvec = CountVectorizer (max_features = 2500,\n",
    "                        ngram_range = (1,1),\n",
    "                        stop_words = None)\n",
    "\n",
    "#Instantiating Bagging Classifier\n",
    "bag = BaggingClassifier(n_estimators = 111)\n",
    "\n",
    "#Transforming our features matrix for this particular method (optimized Count Vectorizer)\n",
    "X_bag_train_cvec = cvec.fit_transform(X_train)\n",
    "X_bag_test_cvec = cvec.transform(X_test)\n",
    "\n",
    "#Fitting the model\n",
    "results_bag = bag.fit(X_bag_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score training set\n",
    "results_bag.score(X_bag_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score testing set\n",
    "results_bag.score(X_bag_test_cvec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, with the Bagging classifier we get the results of accuracy for the training set of 1.0, and just 0.75 for the testing set. It's quite an improvement and so far exactly matches the best testing set accuracy for Multinomial NB model, but still tells that the model is very overfitted on training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Random Forest Classifier ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having witnessed quite some growth with the testing set accuracy score, it's quite natural to want to try a ode advanced classifier on our data, which is a Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "#Fitting the model\n",
    "results_rf_cvec = rf.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9895470383275261"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score training set\n",
    "results_rf_cvec.score(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7395833333333334"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score testing set\n",
    "results_rf_cvec.score(X_test_cvec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, with the Random Forest classifier we get the results of accuracy for the training set of 0.9835, and just 0.7129 for the testing set. They are not the best results comparinng to our Bagging classifier model, but still tells that the model is very overfitted on training data.\n",
    "As another attempt to improve model's performance, let's do a gridsearch on model's parameter n_estimators (number of trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilyakvyatkovskiy/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 54}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting a dictionary of parameters for a grid search\n",
    "rf_params = { 'n_estimators': np.arange(2,60,2) }\n",
    "\n",
    "#Instantiating a grid search with the parameters chosen above and a 5-fold cross-validation\n",
    "gs = GridSearchCV(rf, param_grid=rf_params, cv=5)\n",
    "\n",
    "#Fitting our grid search \n",
    "gs.fit(X_train_cvec, y_train)\n",
    "\n",
    "#Getting our best parameter\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7700348432055749"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting our best score for our best parameter\n",
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting our best training score\n",
    "gs.score(X_train_cvec,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid-searched by number of trees Random Forest classifier with 54 trees gives us the best accuracy score of 0.7700 on a testing set. This is so far the best result. But the model is quite overfitted on training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Extra Trees Classifier ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating an Extra Trees Clasifier with n_estimators = 54\n",
    "et = ExtraTreesClassifier(n_estimators=54)\n",
    "\n",
    "#Fitting the model\n",
    "results_et_cvec = rf.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9790940766550522"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score training set\n",
    "results_et_cvec.score(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score testing set\n",
    "results_et_cvec.score(X_test_cvec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, an Extra Trees Classifier hasn't shown any improvement in comparison with the previously set highest score on a testing set of 0.7700 by the Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 AdaBoost Classifier ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having witnessed another improvement with the Random Forest Classifier with number of trees(n_estimators) optimized at 70 trees, it's quite natural to try an AdaBoost Classification model on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating an AdaBoost Clasifier with Decision Tree Classifier as estimator and n_estimators=60\n",
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), \n",
    "                         n_estimators = 70)\n",
    "\n",
    "#Fitting the model\n",
    "results_ada_cvec = rf.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9825783972125436"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score training set\n",
    "results_ada_cvec.score(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score testing set\n",
    "results_ada_cvec.score(X_test_cvec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost Classifier hasn't worked better on our data, as the result on testing data is slightly lower than for the Extra Tree model, and this model is also highly overfitted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Gradient Boosting Classifier ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating an Gradient Boosting Clasifier \n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "#Fitting the model\n",
    "results_gb_cvec = rf.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9790940766550522"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score training set\n",
    "results_gb_cvec.score(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65625"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score testing set\n",
    "results_gb_cvec.score(X_test_cvec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Classifier hasn't worked better on our data as well, as the result on testing data is lower than for the Random Forest model, and this model is also highly overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Support Vector Machines ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiating our model with default parameters\n",
    "svc = SVC()\n",
    "\n",
    "#Fitting the model\n",
    "svc.fit(X_train_cvec,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.662020905923345"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score training set\n",
    "svc.score(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65625"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score testing set\n",
    "svc.score(X_test_cvec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see, our model's performance is far from ideal. It even performs slightly below our Baseline Model (accuracy of 0.6792) with testing set accuracy slightly out-performing the training set, which indicates some underfitting. \n",
    "Theoretically, Support Vector Classifier should work quite well. Additionally, underfitting might be a sign more regularization is required. I would like to do a grid search on model's regularization parameter C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilyakvyatkovskiy/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Setting a dictionary of parameters for a grid search\n",
    "svc_params = { 'C': np.linspace(0.01,10,100),\n",
    "             'kernel':['rbf','poly' ],\n",
    "             'gamma': ['scale','auto_deprecated' ]}\n",
    "\n",
    "#Instantiating a grid search with the parameters chosen above and a 5-fold cross-validation\n",
    "gs = GridSearchCV(svc, param_grid=svc_params, cv=5)\n",
    "\n",
    "#Fitting our grid search \n",
    "gs.fit(X_train_cvec, y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 3.34, 'gamma': 'scale', 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting our best parameter\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7770034843205574"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting best accuracy score\n",
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9512195121951219"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best score on a training set\n",
    "gs.score(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the Supporting Vectors Machine model works best on our data. We got 0.7770 score on testing set, which is the highest among all the models. But still, this model suffers from overfitting quite a lot as the accuracy score on training set is 0.9512. All attempts to enhance/extend hyperparameters grid search in order to improve model's perforamnce didn't really get any worthwhile change of model's accuracy on testing set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusions #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My best model has turned out to be the Support Vectors Machine, and this is a expected result. This model has shown the highest accuracy on a testing set, and more than 3 out of 4 internet posts could be correctly identified as belonging to one of the two threads.\n",
    "Quite an unexpected part of this result is that the model's performance is quite just a bit better (by just 3.14%) than the model's of the first choice - Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Recommendations #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the current project I'd suggest the following:\n",
    "- Working with bigger dataset, as current entire datasets contained just 383 posts\n",
    "- Experiment with more optimization options - grid searching through more hyperparameters while paying attention to extending/narrowing each parameter's range and iteration steps through the corresponding ranges. _This inflicts quite some computational capacity problems_\n",
    "\n",
    "Generally speaking, if we wanted to significantly improve our model's accuracy classifying pieces of human-produced text we should definitely consider using more advanced algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
